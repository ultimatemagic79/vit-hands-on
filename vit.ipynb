{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyODvIP14v2izkqXSg7fpbmt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ultimatemagic79/vit-hands-on/vit.ipynb)"],"metadata":{"id":"KwHAT84krryr"}},{"cell_type":"code","source":["# Google Driveのマウント\n","from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"km0XhKrjVxm1","executionInfo":{"status":"ok","timestamp":1716952537147,"user_tz":-540,"elapsed":27426,"user":{"displayName":"Kaito Sugiyama","userId":"02456899258560881334"}},"outputId":"2bd64a93-6ab8-475d-f565-7f5edd95c44a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n","[Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebook/vit-hands-on'\n","/content\n"]}]},{"cell_type":"code","source":["%cd '/content/drive/MyDrive/Colab Notebooks/vit-hands-on'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kUxApvRIWPYX","executionInfo":{"status":"ok","timestamp":1716952564578,"user_tz":-540,"elapsed":440,"user":{"displayName":"Kaito Sugiyama","userId":"02456899258560881334"}},"outputId":"6a1ceaff-cc3c-45aa-9f56-5bf634758d7f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/vit-hands-on\n"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"Antvi4BsWRty"}},{"cell_type":"markdown","source":["# Vision Transformerを実装してみよう\n","ViTモデルを実際に実装し， CIFAR10の分類タスクを行ってみましょう．\n","## 1. データセット準備\n","画像データセット[CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html)をダウンロードして，前処理を行う.\n"],"metadata":{"id":"wSwEjC8zuKWw"}},{"cell_type":"code","execution_count":8,"metadata":{"id":"mvqUbdknhjqc","executionInfo":{"status":"ok","timestamp":1716955684643,"user_tz":-540,"elapsed":1,"user":{"displayName":"Kaito Sugiyama","userId":"02456899258560881334"}}},"outputs":[],"source":["# モジュールのインポート\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.transforms as transforms\n","from torchvision.datasets import CIFAR10\n","from torch.utils.data import random_split, DataLoader, Subset\n","import numpy as np\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","source":["# データロード関数を定義\n","## 引数batch_sizeはミニバッチの大きさ\n","\n","def load_data(batch_size):\n","\n","    # クラスのラベル名\n","    classes = ('airplane', 'automobile', 'bird', 'cat',\n","            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","    ## 前処理関数の準備\n","    transform = transforms.Compose([\n","        transforms.Resize((224, 224)),  # ViT に合わせて画像サイズを変更\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    ])\n","\n","    # CIFAR10の準備（ローカルにデータがない場合はダウンロードされる）\n","    # 訓練用データセット\n","    trainset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n","    # 評価用データセット\n","    testset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n","\n","    # 訓練用データローダー\n","    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n","    # 評価用データローダー\n","    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n","\n","    return (trainloader, testloader, classes)"],"metadata":{"id":"fuPKQgpZW5wA","executionInfo":{"status":"ok","timestamp":1716952908024,"user_tz":-540,"elapsed":2,"user":{"displayName":"Kaito Sugiyama","userId":"02456899258560881334"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["trainloader, testloader, classs = load_data(64)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bUUcyBmtXjwo","executionInfo":{"status":"ok","timestamp":1716952960902,"user_tz":-540,"elapsed":19172,"user":{"displayName":"Kaito Sugiyama","userId":"02456899258560881334"}},"outputId":"45fbc824-4cf6-4736-e3c9-f47d63142b73"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170498071/170498071 [00:10<00:00, 16079236.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n"]}]},{"cell_type":"markdown","source":["## 2. VisionTransformerの定義\n","実際に学習するVisionTransformerモデルを定義してみよう．ViTの全体像は以下のようになっており，\n","\n","* Patch Embedding\n","* Positional Encoding\n","* Transformer Encoder\n","* Head\n","\n","の大きく四つを組み立てることで作ることができます．\n","![](https://production-media.paperswithcode.com/methods/Screen_Shot_2021-01-26_at_9.43.31_PM_uI4jjMq.png)\n"],"metadata":{"id":"gp2q6LYYX2Lb"}},{"cell_type":"markdown","source":["### Patch Embedding"],"metadata":{"id":"wsQ8mwG4fwtT"}},{"cell_type":"code","source":["# Patch Embedding\n","class PatchEmbedding(nn.Module):\n","    def __init__():\n","      pass\n","    def forward():\n","      pass"],"metadata":{"id":"s0nKhzf0Xqtg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 例"],"metadata":{"id":"X8IB8H7igBcI"}},{"cell_type":"code","source":["class PatchEmbedding(nn.Module):\n","    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n","        super().__init__()\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.n_patches = (img_size // patch_size) ** 2\n","        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n","\n","    def forward(self, x):\n","        x = self.proj(x)  # (B, C, H, W)\n","        x = x.flatten(2)  # (B, C, N)\n","        x = x.transpose(1, 2)  # (B, N, C)\n","        return x"],"metadata":{"id":"qmMACNjIgEw2","executionInfo":{"status":"ok","timestamp":1716955688598,"user_tz":-540,"elapsed":397,"user":{"displayName":"Kaito Sugiyama","userId":"02456899258560881334"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["### Positional Encoding"],"metadata":{"id":"o4y31PdVfmbh"}},{"cell_type":"code","source":["# Positional Encoding\n","class PositionalEncoding(nn.Module):\n","    def __init__():\n","        pass\n","    def forward():\n","        pass"],"metadata":{"id":"7wN6TGcBaMkq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 例"],"metadata":{"id":"zTlw7cIKgVmw"}},{"cell_type":"code","source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super().__init__()\n","        self.encoding = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        self.encoding[:, 0::2] = torch.sin(position * div_term)\n","        self.encoding[:, 1::2] = torch.cos(position * div_term)\n","        self.encoding = self.encoding.unsqueeze(0)\n","\n","    def forward(self, x):\n","        return x + self.encoding[:, :x.size(1)].detach()"],"metadata":{"id":"-TAA6eMhhoI_","executionInfo":{"status":"ok","timestamp":1716955692568,"user_tz":-540,"elapsed":386,"user":{"displayName":"Kaito Sugiyama","userId":"02456899258560881334"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["### Encoder"],"metadata":{"id":"ZsMwPsnNhwPe"}},{"cell_type":"code","source":["# Transformer Encoder\n","class TransformerEncoder(nn.Module):\n","    def __init__():\n","        pass\n","    def forward():\n","        pass"],"metadata":{"id":"blZ04BuwaZAH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 例"],"metadata":{"id":"JuDBZkash0OC"}},{"cell_type":"code","source":["class TransformerEncoder(nn.Module):\n","    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n","        super().__init__()\n","        self.norm1 = nn.LayerNorm(embed_dim)\n","        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout)\n","        self.norm2 = nn.LayerNorm(embed_dim)\n","        self.ff = nn.Sequential(\n","            nn.Linear(embed_dim, ff_dim),\n","            nn.ReLU(),\n","            nn.Linear(ff_dim, embed_dim),\n","        )\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, src):\n","        src = src + self.attn(self.norm1(src), self.norm1(src), self.norm1(src))[0]\n","        src = src + self.ff(self.norm2(src))\n","        return src"],"metadata":{"id":"9tMhSn7Ah3Li","executionInfo":{"status":"ok","timestamp":1716955697293,"user_tz":-540,"elapsed":386,"user":{"displayName":"Kaito Sugiyama","userId":"02456899258560881334"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["### Vision Transformer"],"metadata":{"id":"pp8pd9fIh7dZ"}},{"cell_type":"code","source":["# Vision Transformer 全体\n","class VisionTransformer(nn.Module):\n","    def __init__():\n","        pass\n","    def forward():\n","        pass"],"metadata":{"id":"068DrS-Nahmm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 例"],"metadata":{"id":"y73OpK76h-YJ"}},{"cell_type":"code","source":["class VisionTransformer(nn.Module):\n","    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, num_heads=12, ff_dim=3072, num_layers=12):\n","        super().__init__()\n","        self.patch_embed = PatchEmbedding(img_size, patch_size, in_chans, embed_dim)\n","        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n","        self.pos_embed = PositionalEncoding(embed_dim)\n","        self.transformer = nn.ModuleList([\n","            TransformerEncoder(embed_dim, num_heads, ff_dim) for _ in range(num_layers)\n","        ])\n","        self.head = nn.Linear(embed_dim, 1000)  # assuming 1000 classes\n","\n","    def forward(self, x):\n","        x = self.patch_embed(x)\n","        b, n, _ = x.shape\n","        cls_tokens = self.cls_token.expand(b, -1, -1)\n","        x = torch.cat((cls_tokens, x), dim=1)\n","        x = self.pos_embed(x)\n","        for layer in self.transformer:\n","            x = layer(x)\n","        x = self.head(x[:, 0])\n","        return x"],"metadata":{"id":"2TwQ2JFih_w4","executionInfo":{"status":"ok","timestamp":1716955700222,"user_tz":-540,"elapsed":515,"user":{"displayName":"Kaito Sugiyama","userId":"02456899258560881334"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["## 3. モデルの学習"],"metadata":{"id":"SO8TTJcOiPLZ"}},{"cell_type":"code","source":["# 訓練関数\n","def train(model, data_loader, criterion, optimizer, device):\n","    model.train()\n","    total_loss = 0\n","    correct = 0\n","    for images, labels in data_loader:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        # モデルで予測\n","        outputs = model(images)\n","\n","        # 損失計算\n","        loss = criterion(outputs, labels)\n","        total_loss += loss.item()\n","\n","        # 逆伝播\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # 正解数を計算\n","        _, predicted = outputs.max(1)\n","        correct += predicted.eq(labels).sum().item()\n","\n","    avg_loss = total_loss / len(data_loader)\n","    accuracy = correct / len(data_loader.dataset)\n","    return avg_loss, accuracy\n","\n","# 評価関数\n","def evaluate(model, data_loader, criterion, device):\n","    model.eval()\n","    total_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for images, labels in data_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            correct += predicted.eq(labels).sum().item()\n","\n","    avg_loss = total_loss / len(data_loader)\n","    accuracy = correct / len(data_loader.dataset)\n","    return avg_loss, accuracy\n"],"metadata":{"id":"62eI23r-iOaZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 定義\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = VisionTransformer().to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","criterion = nn.CrossEntropyLoss()"],"metadata":{"id":"BPYzgmUqjlPM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 訓練\n","num_epochs = 10\n","for epoch in range(num_epochs):\n","    train_loss, train_accuracy = train(model, trainloader, criterion, optimizer, device)\n","    test_loss, test_accuracy = evaluate(model, testloader, criterion, device)\n","    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '\n","          f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n"],"metadata":{"id":"xq_aJFUujtcS"},"execution_count":null,"outputs":[]}]}